{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import neptune\n",
    "import syft as sy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cvxpy as cp\n",
    "from time import sleep\n",
    "from copy import deepcopy\n",
    "import torchattacks\n",
    "import torchvision\n",
    "import coloredlogs, logging\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "from torch.nn import functional as F\n",
    "from federated_learning.FLNet import FLNet\n",
    "from federated_learning.FLCustomDataset import FLCustomDataset\n",
    "from federated_learning.Arguments import Arguments\n",
    "from federated_learning.helper import utils\n",
    "import time\n",
    "\n",
    "CONFIG_PATH = '../configs/defaults.yml'\n",
    "TQDM_R_BAR = '{l_bar}{bar}| {n_fmt}/{total_fmt} [{postfix}] '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs:\n",
      "        Epoch:\t5\n",
      "        Rounds:\t500\n",
      "        Total Number of Users:\t100\n",
      "        Selected Users:\t30\n",
      "        Mode:\tavg\n",
      "        Attack:\t1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "arguments = dict()\n",
    "arguments['--log'] = False\n",
    "arguments['--nep-log'] = False\n",
    "arguments['--avg'] = True\n",
    "arguments['--opt'] = False\n",
    "\n",
    "configs = utils.load_config(CONFIG_PATH)\n",
    "\n",
    "args = Arguments(\n",
    "    configs['runtime']['batch_size'],\n",
    "    configs['runtime']['test_batch_size'],\n",
    "    configs['runtime']['rounds'],\n",
    "    configs['runtime']['epochs'],\n",
    "    configs['runtime']['lr'],\n",
    "    configs['runtime']['momentum'],\n",
    "    configs['runtime']['weight_decay'],\n",
    "    configs['mnist']['shards_num'],\n",
    "    configs['mnist']['shards_per_worker_num'],\n",
    "    configs['mnist']['total_users_num'],\n",
    "    configs['mnist']['selected_users_num'],\n",
    "    configs['server']['data_fraction'],\n",
    "    \"avg\" if arguments['--avg'] else \"opt\",\n",
    "    configs['attack']['attack_type'],\n",
    "    configs['attack']['attackers_num'],\n",
    "    configs['runtime']['use_cuda'],\n",
    "    torch.device(\"cuda\" if configs['runtime']['use_cuda'] else \"cpu\"),\n",
    "    configs['runtime']['random_seed'],\n",
    "    configs['log']['interval'],\n",
    "    configs['log']['level'],\n",
    "    configs['log']['format'],\n",
    "    utils.make_output_dir(\n",
    "        configs['log']['root_output_dir'], arguments['--output-prefix']\n",
    "        ) if arguments['--log'] else \"\",\n",
    "    True if arguments['--nep-log'] else False,\n",
    "    True if arguments['--log'] else False\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "coloredlogs.install(level=args.log_level, fmt=args.log_format)\n",
    "\n",
    "print(\n",
    "        \"Configs:\\n\\\n",
    "        Epoch:\\t{}\\n\\\n",
    "        Rounds:\\t{}\\n\\\n",
    "        Total Number of Users:\\t{}\\n\\\n",
    "        Selected Users:\\t{}\\n\\\n",
    "        Mode:\\t{}\\n\\\n",
    "        Attack:\\t{}\".format(\n",
    "            args.epochs, args.rounds, args.total_users_num, args.selected_users_num, \n",
    "            args.mode, args.attack_type, args.attackers_num\n",
    "        ))\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "# syft initialization\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_model = FLNet().to(args.device)\n",
    "# server_model.load_state_dict(torch.load(\"/home/savi/ehsan/FederatedLearning/data/R290_server_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workers(hook, workers_idx):\n",
    "    logging.info(\"Creating {} workers...\".format(len(workers_idx)))\n",
    "    workers = dict()\n",
    "    for worker_id in workers_idx:\n",
    "        logging.debug(\"Creating the worker: {}\".format(worker_id))\n",
    "        workers[worker_id] = sy.VirtualWorker(hook, id=worker_id)\n",
    "    logging.info(\"Creating {} workers..... OK\".format(len(workers_idx)))\n",
    "    return workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 22:45:34: Total number of users: 100\n",
      "2021-02-04 22:45:34: Creating 100 workers...\n",
      "2021-02-04 22:45:34: Creating 100 workers..... OK\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Total number of users: {}\".format(args.total_users_num))\n",
    "workers_idx = [\"worker_\" + str(i) for i in range(args.total_users_num)]\n",
    "workers = create_workers(hook, workers_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 22:45:35: Loading train data from MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = utils.load_mnist_dataset(\n",
    "        train=True, transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 22:45:36: Sorting the MNIST dataset based on labels...\n",
      "2021-02-04 22:45:38: Splitting the dataset into tensors with 300 samples...\n",
      "2021-02-04 22:45:38: Federated data to 100 users..... OK\n",
      "2021-02-04 22:45:38: Extracting 500.0 of users data (total: 300000) to be sent to the server...\n",
      "2021-02-04 22:45:38: Extracted... Ok, The size of the extracted data: torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "sorted_train_dataset = utils.sort_mnist_dataset(train_dataset)\n",
    "splitted_train_dataset = utils.split_dataset(\n",
    "    sorted_train_dataset, int(len(sorted_train_dataset) / args.shards_num))\n",
    "mapped_train_datasets = utils.map_shards_to_worker(splitted_train_dataset, workers_idx, args.shards_per_worker_num)\n",
    "\n",
    "server_dataset = utils.fraction_of_datasets(mapped_train_datasets, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 22:46:11: Extracting 5.0 of users data (total: 3000) to be sent to the server...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7ce7131c956b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mserver_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfraction_of_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped_train_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_data_fraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ehsan/FederatedLearning/federated_learning/helper/utils.py\u001b[0m in \u001b[0;36mfraction_of_datasets\u001b[0;34m(datasets, fraction)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mww_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ehsan/venv/src/syft/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mroute_method_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# means that there is a wrapper to remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ehsan/venv/src/syft/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "server_dataset = utils.fraction_of_datasets(mapped_train_datasets, args.server_data_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 22:55:20: Extracting 5.0% of users data (total: 3000) to be sent to the server...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([213,  84, 324, 491, 137, 120, 374, 129, 486, 325, 241, 481, 311,  20,\n",
      "        562,  19, 424, 280, 149, 124, 243, 323,  68,  11, 568, 454, 548, 489,\n",
      "        512,  12]) <class 'torch.Tensor'>\n",
      "30\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "datasets = mapped_train_datasets\n",
    "fraction = 0.05\n",
    "logging.info(\"Extracting {}% of users data (total: {}) to be sent to the server...\".format(\n",
    "    fraction * 100.0, int(fraction * len(datasets) * len(list(datasets.values())[0].targets))))\n",
    "images, labels = [], []\n",
    "for ww_id, dataset in datasets.items():\n",
    "    idx = torch.randperm(len(dataset.targets))[:int(fraction * len(dataset.targets))]\n",
    "    print(idx, type(idx))\n",
    "    print(len(dataset.data[idx.tolist()]))\n",
    "    print(dataset.targets[idx.tolist()])\n",
    "#     print(len(dataset.targets), int(fraction * len(dataset.targets)))\n",
    "#     images.append(dataset.data[list(idx)])\n",
    "#     labels.append(dataset.targets[(idx)])\n",
    "    break\n",
    "\n",
    "# aggregate_dataset = FLCustomDataset(\n",
    "#     cat(images), cat(labels),\n",
    "#     transform=transforms.Compose([\n",
    "#         transforms.ToTensor()])\n",
    "# )\n",
    "# logging.info(\"Extracted... Ok, The size of the extracted data: {}\".format(\n",
    "#     aggregate_dataset.data.shape))\n",
    "# return aggregate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(server_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = utils.load_mnist_dataset(\n",
    "        train=False, transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),]))\n",
    "test_loader = utils.get_dataloader(\n",
    "    test_dataset, args.test_batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, round_no, args, atk=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with tqdm(total=len(test_loader), ncols=80, leave=False, desc=\"Test\\t\", bar_format=TQDM_R_BAR) as t1:\n",
    "        for jj, (data, target) in enumerate(test_loader):\n",
    "            if atk is not None:\n",
    "                data = atk(data, target)\n",
    "                data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n",
    "            else:\n",
    "                data, target = data.to(args.device), target.to(args.device, dtype=torch.int64)\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "                pred = output.argmax(1, keepdim=True)  # get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            t1.update()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    if args.neptune_log:\n",
    "        neptune.log_metric(\"test_loss\", test_loss)\n",
    "        neptune.log_metric(\"test_acc\", test_acc)\n",
    "    if args.local_log:\n",
    "        file = open(args.log_dir +  \"accuracy\", \"a\")\n",
    "        TO_FILE = '{} {} \"{{/*Accuracy:}}\\\\n{}%\" {}\\n'.format(\n",
    "            round_no, test_loss, test_acc, test_acc)\n",
    "        file.write(TO_FILE)\n",
    "        file.close()\n",
    "    \n",
    "    logging.debug('Test Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), test_acc))\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(server_model, test_loader, 1, args)\n",
    "print('Test Average loss: {:.4f}, Accuracy: {:.0f}%'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchattacks\n",
    "atks = [torchattacks.PGD(server_model, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.BIM(server_model, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.CW(server_model, c=1, kappa=0, steps=1000, lr=0.01),\n",
    "        torchattacks.RFGSM(server_model, eps=8/255, alpha=4/255, steps=1),\n",
    "        torchattacks.FGSM(server_model, eps=8/255),\n",
    "        torchattacks.FFGSM(server_model, eps=8/255, alpha=12/255),\n",
    "        torchattacks.TPGD(server_model, eps=8/255, alpha=2/255, steps=7),\n",
    "        torchattacks.MIFGSM(server_model, eps=8/255, decay=1.0, steps=5),\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for atk in atks :\n",
    "    print(\"-\"*70)\n",
    "    print(atk)\n",
    "    start = time.time()\n",
    "    test_loss, test_acc = test(server_model, test_loader, 1, args, atk)\n",
    "    print('Test Average loss: {:.4f}, Accuracy: {:.0f}%, Time: {}'.format(test_loss, test_acc, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = torchattacks.FGSM(server_model, eps=0.06)\n",
    "print(\"-\"*70)\n",
    "print(atk)\n",
    "start = time.time()\n",
    "test_loss, test_acc = test(server_model, test_loader, 1, args, atk)\n",
    "print('Test Average loss: {:.4f}, Accuracy: {:.0f}%, Time: {}'.format(test_loss, test_acc, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = torchattacks.FGSM(server_model, eps=0.06)\n",
    "print(\"-\"*70)\n",
    "print(atk)\n",
    "start = time.time()\n",
    "test_loss, test_acc = test(server_model, test_loader, 1, args, atk)\n",
    "print('Test Average loss: {:.4f}, Accuracy: {:.0f}%, Time: {}'.format(test_loss, test_acc, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = torchattacks.FGSM(server_model, eps=0.09)\n",
    "print(\"-\"*70)\n",
    "print(atk)\n",
    "start = time.time()\n",
    "test_loss, test_acc = test(server_model, test_loader, 1, args, atk)\n",
    "print('Test Average loss: {:.4f}, Accuracy: {:.0f}%, Time: {}'.format(test_loss, test_acc, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = torchattacks.FGSM(server_model, eps=0.25)\n",
    "print(\"-\"*70)\n",
    "print(atk)\n",
    "start = time.time()\n",
    "test_loss, test_acc = test(server_model, test_loader, 1, args, atk)\n",
    "print('Test Average loss: {:.4f}, Accuracy: {:.0f}%, Time: {}'.format(test_loss, test_acc, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atk = torchattacks.FGSM(server_model, eps=0.25)\n",
    "# data_ = atk(data, target)\n",
    "data_ = data.numpy()\n",
    "target_ = target.numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(target_[i])\n",
    "    plt.imshow(data_[i][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = torchattacks.FGSM(server_model, eps=0.10)\n",
    "data_ = atk(data, target)\n",
    "data_ = data_.numpy()\n",
    "target_ = target.numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(target_[i])\n",
    "    plt.imshow(data_[i][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = torchattacks.FGSM(server_model, eps=0.20)\n",
    "data_ = atk(data, target)\n",
    "data_ = data_.numpy()\n",
    "target_ = target.numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(target_[i])\n",
    "    plt.imshow(data_[i][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = torchattacks.FGSM(server_model, eps=0.25)\n",
    "data_ = atk(data, target)\n",
    "data_ = data_.numpy()\n",
    "target_ = target.numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(target_[i])\n",
    "    plt.imshow(data_[i][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(test_loader))\n",
    "\n",
    "worker = sy.VirtualWorker(hook, id=\"worker2\")\n",
    "\n",
    "data = data.send(worker)\n",
    "target = target.send(worker)\n",
    "\n",
    "print(target.location)\n",
    "print(target)\n",
    "tt = target.get()\n",
    "print(tt)\n",
    "\n",
    "# atk = torchattacks.FGSM(server_model, eps=0.25)\n",
    "# data, target = data.get(), target.get()\n",
    "# data_ = atk(data, target)\n",
    "# data_ = data_.numpy()\n",
    "# target_ = target.numpy()\n",
    "# print(data_.shape)\n",
    "# print(target_.shape)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# figure = plt.figure(figsize=(8,8))\n",
    "# for i in range(20):\n",
    "#     plt.subplot(4, 5, i + 1)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(target_[i])\n",
    "#     plt.imshow(data_[i][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
