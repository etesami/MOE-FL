{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-20 16:24:15,476 Initializing Federated Learning class...\n",
      "2020-12-20 16:24:15,865 Loading train dataset from /home/ubuntu/data/leaf_non_iid/data/femnist/data\n",
      "2020-12-20 16:24:15,866 Loading 1 out of 4 files...\n",
      "2020-12-20 16:24:18,803 Loading 2 out of 4 files...\n",
      "2020-12-20 16:24:21,691 Loading 3 out of 4 files...\n",
      "2020-12-20 16:24:25,354 Loading 4 out of 4 files...\n",
      "2020-12-20 16:24:28,283 Start processing of femnist data...\n",
      "2020-12-20 16:24:30,788 Loading test dataset from /home/ubuntu/data/leaf_non_iid/data/femnist/data\n",
      "2020-12-20 16:24:30,790 Loading 1 out of 4 files...\n",
      "2020-12-20 16:24:31,070 Loading 2 out of 4 files...\n",
      "2020-12-20 16:24:31,360 Loading 3 out of 4 files...\n",
      "2020-12-20 16:24:31,729 Loading 4 out of 4 files...\n",
      "2020-12-20 16:24:32,021 Start processing of femnist data...\n",
      "2020-12-20 16:24:32,165 Total of 31 workers are in the dataset.\n",
      "2020-12-20 16:24:32,167 Select 30 workers to be used from the dataset.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import logging\n",
    "import random\n",
    "import neptune\n",
    "import numpy as np\n",
    "import syft as sy\n",
    "from torch import load\n",
    "from torchvision import transforms\n",
    "from federated_learning.FLCustomDataset import FLCustomDataset\n",
    "from federated_learning.FederatedLearning import FederatedLearning\n",
    "from federated_learning.helper import utils\n",
    "\n",
    "CONFIG_PATH = '../configs/defaults.yml'\n",
    "\n",
    "\n",
    "configs = utils.load_config(CONFIG_PATH)\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=configs['log']['level'])\n",
    "random.seed(configs['runtime']['random_seed'])\n",
    "\n",
    "# Logging initialization\n",
    "log_enable = False\n",
    "output_dir = None\n",
    "if log_enable:\n",
    "    output_dir = utils.make_output_dir(\n",
    "        configs['log']['root_output_dir'], arguments['--output-prefix'])\n",
    "    utils.save_configs(output_dir, configs)\n",
    "neptune_enable = False\n",
    "\n",
    "epochs_num = configs['runtime']['epochs']\n",
    "rounds_num = configs['runtime']['rounds']\n",
    "\n",
    "fl = FederatedLearning(\n",
    "    configs['runtime']['batch_size'], \n",
    "    configs['runtime']['test_batch_size'], \n",
    "    configs['runtime']['lr'], \n",
    "    configs['runtime']['reg'],\n",
    "    configs['runtime']['momentum'], \n",
    "    neptune_enable, log_enable, \n",
    "    configs['log']['interval'], \n",
    "    output_dir, \n",
    "    configs['runtime']['random_seed'])\n",
    "\n",
    "\n",
    "raw_train_data = utils.preprocess_leaf_data(\n",
    "    utils.load_leaf_train(configs['data']['FEMNIST_PATH']), only_digits=True\n",
    ")\n",
    "raw_test_data = utils.preprocess_leaf_data(\n",
    "    utils.load_leaf_test(configs['data']['FEMNIST_PATH']), min_num_samples=configs['runtime']['test_batch_size'], only_digits=True\n",
    ")\n",
    "\n",
    "# common users in processed test/train dataset\n",
    "workers_idx_all = sorted(list(set(raw_test_data.keys()).intersection(raw_train_data.keys())))\n",
    "logging.info(\"Total of {} workers are in the dataset.\".format(len(workers_idx_all)))\n",
    "\n",
    "workers_idx_to_be_used = utils.get_workers_idx(\n",
    "    workers_idx_all,\n",
    "    configs['runtime']['femnist_users_num'],\n",
    "    []\n",
    ")\n",
    "logging.info(\"Select {} workers to be used from the dataset.\".format(len(workers_idx_to_be_used)))\n",
    "\n",
    "\n",
    "trusted_idx = utils.get_workers_idx(\n",
    "    workers_idx_to_be_used, configs['runtime']['femnist_trusted_num'], [])\n",
    "eavesdroppers_idx = utils.get_workers_idx(\n",
    "    workers_idx_to_be_used, configs['runtime']['femnist_eavesdropper_num'], trusted_idx)\n",
    "normal_idx = utils.get_workers_idx(\n",
    "    workers_idx_to_be_used, \n",
    "    len(workers_idx_to_be_used) - \n",
    "    (int(configs['runtime']['femnist_eavesdropper_num']) + int(configs['runtime']['femnist_trusted_num'])),\n",
    "    eavesdroppers_idx + trusted_idx)\n",
    "\n",
    "# logging.info(\"Trusted [{}]: {}\".format(len(trusted_idx), trusted_idx))\n",
    "# logging.info(\"Eavesdroppers [{}]: {}\".format(len(eavesdroppers_idx), eavesdroppers_idx))\n",
    "# logging.info(\"Normal [{}]: {}\".format(len(normal_idx), normal_idx))\n",
    "# if log_enable:\n",
    "#     utils.write_to_file(output_dir, \"all_users\", workers_idx_all)\n",
    "#     utils.write_to_file(output_dir, \"eavesdroppers\", eavesdroppers_idx)\n",
    "#     utils.write_to_file(output_dir, \"normal\", normal_idx)\n",
    "#     utils.write_to_file(output_dir, \"trusted\", trusted_idx)\n",
    "\n",
    "# # fl.create_server()\n",
    "# # fl.create_server_model()\n",
    "# # fl.create_workers(workers_idx_to_be_used)\n",
    "# # fl.create_workers_model(workers_idx_to_be_used)\n",
    "    \n",
    "# # # Create test dataloader from all normal and eveasdroppers\n",
    "# # fed_test_dataloader = fl.create_femnist_server_test_dataloader(\n",
    "# #     raw_test_data, workers_idx_to_be_used)\n",
    "\n",
    "# # # W0 model\n",
    "# # # trained_w0_model = load(configs['runtime']['W0_pure_path'])\n",
    "# # fed_train_datasets = None\n",
    "# # if arguments[\"--no-attack\"]:\n",
    "# #     logging.info(\"No Attack will be performed.\")\n",
    "# #     fed_train_datasets = fl.create_femnist_train_datasets(raw_train_data, workers_idx_to_be_used)\n",
    "# # elif arguments[\"--attack\"] == \"999\": # Combines\n",
    "# #     logging.info(\"Perform combined attacks 1, 2, 3\")\n",
    "# #     dataset = utils.perfrom_attack_femnist(\n",
    "# #             raw_train_data, 1, workers_idx_to_be_used, eavesdroppers_idx)\n",
    "# #     dataset = utils.perfrom_attack_femnist(\n",
    "# #             dataset, 2, workers_idx_to_be_used, eavesdroppers_idx)\n",
    "# #     dataset = utils.perfrom_attack_femnist(\n",
    "# #             dataset, 3, workers_idx_to_be_used, eavesdroppers_idx)\n",
    "# #     fed_train_datasets = fl.create_femnist_train_datasets(dataset, workers_idx_to_be_used)\n",
    "# # else:\n",
    "# #     logging.info(\"Perform attack type: {}\".format(arguments[\"--attack\"]))\n",
    "# #     fed_train_datasets = fl.create_femnist_train_datasets(\n",
    "# #         utils.perfrom_attack_femnist(\n",
    "# #             raw_train_data, \n",
    "# #             int(arguments[\"--attack\"]),\n",
    "# #             workers_idx_to_be_used,\n",
    "# #             eavesdroppers_idx\n",
    "# #         ), workers_idx_to_be_used)\n",
    "\n",
    "# # fed_train_dataloaders = dict()\n",
    "# # for ww_id, fed_dataset in fed_train_datasets.items():\n",
    "# #     dataloader = sy.FederatedDataLoader(\n",
    "# #         fed_dataset, batch_size=configs['runtime']['batch_size'], shuffle=False, drop_last=True)\n",
    "# #     fed_train_dataloaders[fed_dataset.workers[0]] = dataloader\n",
    "\n",
    "# # for round_no in range(rounds_num):\n",
    "# #     for counter, worker_id in enumerate(workers_idx_to_be_used):\n",
    "# #         logging.info(\"Training worker {} out of {} workers...\".format(\n",
    "# #             counter+1, len(workers_idx_to_be_used)))\n",
    "# #         fl.train_workers(fed_train_dataloaders[worker_id], [worker_id], round_no, epochs_num)\n",
    "\n",
    "# #     # Find the best weights and update the server model\n",
    "# #     weights = None\n",
    "# #     if arguments['--avg']:\n",
    "# #         weights = [1.0 / len(workers_idx_to_be_used)] * len(workers_idx_to_be_used)\n",
    "# #     elif arguments['--opt']:\n",
    "# #         trusted_weights = [1.0 / len(trusted_idx)] * len(trusted_idx)\n",
    "# #         avg_trusted_model = fl.wieghted_avg_model(trusted_weights, trusted_idx)\n",
    "# #         weights = fl.find_best_weights(avg_trusted_model, normal_idx + eavesdroppers_idx)\n",
    "\n",
    "# #     if log_enable:\n",
    "# #         fl.save_workers_model(workers_idx_to_be_used, str(round_no))\n",
    "# #         # fl.save_model(\n",
    "# #         #     fl.get_average_model(trusted_idx),\n",
    "# #         #     \"R{}_{}\".format(round_no, \"avg_trusted_model\")\n",
    "# #         # )\n",
    "\n",
    "# #     weighted_avg_model = fl.wieghted_avg_model(weights, normal_idx + eavesdroppers_idx)\n",
    "# #     # Update the server model\n",
    "# #     fl.update_models(workers_idx_to_be_used, weighted_avg_model)\n",
    "\n",
    "# #     # Apply the server model to the test dataset\n",
    "# #     fl.test(weighted_avg_model, fed_test_dataloader, round_no)\n",
    "\n",
    "# #     if log_enable:\n",
    "# #         fl.save_model(\n",
    "# #             weighted_avg_model, \n",
    "# #             \"R{}_{}\".format(round_no, \"weighted_avg_model\")\n",
    "# #         )\n",
    "\n",
    "# #     print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-20 16:27:11,403 Loading train dataset from /home/ubuntu/data/leaf_non_iid/data/femnist/data\n",
      "2020-12-20 16:27:11,405 Loading 1 out of 4 files...\n",
      "2020-12-20 16:27:14,388 Loading 2 out of 4 files...\n",
      "2020-12-20 16:27:17,246 Loading 3 out of 4 files...\n",
      "2020-12-20 16:27:20,306 Loading 4 out of 4 files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104:\t1\n",
      "113:\t1\n",
      "128:\t1\n",
      "135:\t1\n",
      "150:\t1\n",
      "154:\t1\n",
      "159:\t1\n",
      "163:\t1\n",
      "169:\t1\n",
      "176:\t1\n",
      "185:\t1\n",
      "186:\t1\n",
      "189:\t1\n",
      "195:\t1\n",
      "198:\t2\n",
      "205:\t3\n",
      "210:\t1\n",
      "211:\t2\n",
      "212:\t1\n",
      "215:\t1\n",
      "216:\t2\n",
      "217:\t1\n",
      "219:\t3\n",
      "220:\t2\n",
      "221:\t2\n",
      "222:\t2\n",
      "223:\t3\n",
      "224:\t2\n",
      "226:\t1\n",
      "228:\t2\n",
      "230:\t1\n",
      "231:\t2\n",
      "232:\t1\n",
      "233:\t2\n",
      "234:\t2\n",
      "236:\t1\n",
      "237:\t2\n",
      "238:\t1\n",
      "239:\t2\n",
      "241:\t1\n",
      "242:\t2\n",
      "243:\t6\n",
      "244:\t1\n",
      "246:\t1\n",
      "247:\t1\n",
      "248:\t1\n",
      "249:\t4\n",
      "250:\t2\n",
      "251:\t2\n",
      "252:\t2\n",
      "253:\t2\n",
      "254:\t2\n",
      "255:\t1\n",
      "256:\t2\n",
      "257:\t4\n",
      "258:\t2\n",
      "259:\t1\n",
      "260:\t4\n",
      "261:\t3\n",
      "262:\t3\n",
      "263:\t2\n",
      "265:\t2\n",
      "266:\t2\n",
      "267:\t1\n",
      "268:\t4\n",
      "269:\t2\n",
      "270:\t4\n",
      "271:\t3\n",
      "272:\t5\n",
      "273:\t2\n",
      "274:\t2\n",
      "275:\t5\n",
      "276:\t1\n",
      "277:\t2\n",
      "278:\t3\n",
      "279:\t1\n",
      "280:\t1\n",
      "282:\t1\n",
      "283:\t4\n",
      "285:\t1\n",
      "286:\t6\n",
      "287:\t3\n",
      "288:\t5\n",
      "289:\t1\n",
      "290:\t2\n",
      "291:\t2\n",
      "292:\t5\n",
      "293:\t4\n",
      "295:\t2\n",
      "296:\t1\n",
      "297:\t5\n",
      "298:\t6\n",
      "299:\t1\n",
      "300:\t2\n",
      "301:\t5\n",
      "302:\t3\n",
      "303:\t3\n",
      "305:\t4\n",
      "306:\t3\n",
      "307:\t4\n",
      "308:\t1\n",
      "309:\t5\n",
      "310:\t6\n",
      "311:\t2\n",
      "312:\t7\n",
      "313:\t3\n",
      "315:\t6\n",
      "316:\t8\n",
      "317:\t4\n",
      "318:\t3\n",
      "319:\t3\n",
      "320:\t4\n",
      "322:\t1\n",
      "323:\t2\n",
      "324:\t9\n",
      "326:\t2\n",
      "327:\t4\n",
      "328:\t2\n",
      "329:\t2\n",
      "330:\t6\n",
      "331:\t7\n",
      "332:\t3\n",
      "333:\t4\n",
      "334:\t1\n",
      "335:\t4\n",
      "337:\t4\n",
      "338:\t2\n",
      "339:\t1\n",
      "340:\t3\n",
      "341:\t4\n",
      "342:\t5\n",
      "345:\t4\n",
      "347:\t3\n",
      "348:\t1\n",
      "349:\t3\n",
      "350:\t1\n",
      "351:\t4\n",
      "352:\t1\n",
      "353:\t3\n",
      "354:\t4\n",
      "355:\t2\n",
      "356:\t3\n",
      "357:\t2\n",
      "358:\t3\n",
      "359:\t3\n",
      "360:\t2\n",
      "361:\t1\n",
      "362:\t3\n",
      "363:\t3\n",
      "366:\t1\n",
      "368:\t2\n",
      "369:\t1\n",
      "372:\t3\n",
      "373:\t1\n",
      "374:\t2\n",
      "375:\t1\n",
      "378:\t1\n",
      "380:\t1\n",
      "383:\t1\n",
      "393:\t1\n",
      "Mean num of samples/user: 282.0\n",
      "Total Samples:\t116969\n",
      "Total Users:\t399\n",
      "[f0640_19]: Images: 287, Pixels: 784\n",
      "mean: 0.9637806415557861\n",
      "std: 0.1600511372089386,\n",
      "max: 1.0\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "utils.dataset_info(utils.load_leaf_train(configs['data']['FEMNIST_PATH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "aa = torch.tensor([1,2,3,4,5], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-20 16:24:51,366 Extract data from raw data for 30 of users...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101:\t2\n",
      "102:\t4\n",
      "103:\t2\n",
      "104:\t1\n",
      "105:\t2\n",
      "106:\t2\n",
      "107:\t3\n",
      "108:\t3\n",
      "109:\t1\n",
      "110:\t4\n",
      "111:\t2\n",
      "112:\t3\n",
      "115:\t1\n",
      "Mean num of samples/user: 107.0\n",
      "Total Samples:\t3209\n",
      "Total Users:\t30\n",
      "[f0824_18]: Images: 107, Pixels: 28\n",
      "mean: 0.9666635394096375\n",
      "std: 0.15451721847057343,\n",
      "max: 1.0\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "utils.dataset_info(utils.extract_data(raw_train_data, workers_idx_to_be_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-20 16:30:29,051 Extract data from raw data for 30 of users...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:\t12\n",
      "16:\t6\n",
      "17:\t5\n",
      "18:\t3\n",
      "19:\t3\n",
      "20:\t1\n",
      "Mean num of samples/user: 18.0\n",
      "Total Samples:\t492\n",
      "Total Users:\t30\n",
      "[f0629_39]: Images: 15, Pixels: 28\n",
      "mean: 0.9668231010437012\n",
      "std: 0.15408051013946533,\n",
      "max: 1.0\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "utils.dataset_info(utils.extract_data(raw_test_data, workers_idx_to_be_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f0840_37', 'f0933_38', 'f0710_08', 'f0898_29', 'f0666_20', 'f0625_26', 'f0843_06', 'f0958_28', 'f0590_06', 'f0519_46', 'f0862_25', 'f0877_17', 'f0864_41', 'f0931_37', 'f0806_09', 'f0836_28', 'f0629_39', 'f0561_12', 'f0701_21', 'f0824_18', 'f0644_19', 'f0792_07', 'f0841_10', 'f0504_29', 'f0617_30', 'f0512_15', 'f0726_09', 'f0834_03', 'f0673_28', 'f0566_02', 'f0539_34']\n",
      "\n",
      "['f0840_37', 'f0933_38', 'f0710_08', 'f0898_29', 'f0666_20', 'f0625_26', 'f0843_06', 'f0958_28', 'f0590_06', 'f0519_46', 'f0862_25', 'f0877_17', 'f0864_41', 'f0931_37', 'f0806_09', 'f0836_28', 'f0629_39', 'f0561_12', 'f0701_21', 'f0824_18', 'f0644_19', 'f0792_07', 'f0841_10', 'f0504_29', 'f0617_30', 'f0512_15', 'f0726_09', 'f0834_03', 'f0673_28', 'f0566_02', 'f0539_34']\n"
     ]
    }
   ],
   "source": [
    "w1 = list(set(raw_test_data.keys()).intersection(raw_train_data.keys()))\n",
    "w2 = list(set(raw_test_data.keys()).intersection(raw_train_data.keys()))\n",
    "print(\"{}\\n\\n{}\".format(w1, w2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100:\t5\n",
      "101:\t14\n",
      "102:\t11\n",
      "103:\t11\n",
      "104:\t12\n",
      "105:\t9\n",
      "106:\t15\n",
      "107:\t12\n",
      "108:\t20\n",
      "109:\t10\n",
      "110:\t12\n",
      "111:\t15\n",
      "112:\t8\n",
      "113:\t4\n",
      "114:\t6\n",
      "115:\t10\n",
      "116:\t5\n",
      "117:\t5\n",
      "118:\t2\n",
      "119:\t3\n",
      "120:\t1\n",
      "121:\t1\n",
      "Mean num of samples/user: 110.0\n",
      "Total Samples:\t20652\n",
      "Total Users:\t191\n",
      "[f0640_19]: Images: 112, Pixels: 28\n",
      "mean: 0.9643265008926392\n",
      "std: 0.1592288762331009,\n",
      "max: 1.0\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "utils.dataset_info(raw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:\t16\n",
      "16:\t13\n",
      "17:\t9\n",
      "18:\t7\n",
      "19:\t4\n",
      "20:\t1\n",
      "Mean num of samples/user: 18.0\n",
      "Total Samples:\t823\n",
      "Total Users:\t50\n",
      "[f0629_39]: Images: 15, Pixels: 28\n",
      "mean: 0.9646655321121216\n",
      "std: 0.15850037336349487,\n",
      "max: 1.0\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "utils.dataset_info(raw_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: [31]: ['f0840_37', 'f0933_38', 'f0710_08', 'f0898_29', 'f0666_20', 'f0625_26', 'f0843_06', 'f0958_28', 'f0590_06', 'f0519_46', 'f0862_25', 'f0877_17', 'f0864_41', 'f0931_37', 'f0806_09', 'f0836_28', 'f0629_39', 'f0561_12', 'f0701_21', 'f0824_18', 'f0644_19', 'f0792_07', 'f0841_10', 'f0504_29', 'f0617_30', 'f0512_15', 'f0726_09', 'f0834_03', 'f0673_28', 'f0566_02', 'f0539_34']\n"
     ]
    }
   ],
   "source": [
    "print(\"All: [{}]: {}\".format(len(workers_idx_all), workers_idx_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Be Used: [30]: ['f0931_37', 'f0504_29', 'f0840_37', 'f0726_09', 'f0512_15', 'f0519_46', 'f0834_03', 'f0877_17', 'f0566_02', 'f0843_06', 'f0590_06', 'f0701_21', 'f0625_26', 'f0898_29', 'f0561_12', 'f0644_19', 'f0824_18', 'f0539_34', 'f0710_08', 'f0629_39', 'f0666_20', 'f0862_25', 'f0806_09', 'f0841_10', 'f0792_07', 'f0617_30', 'f0673_28', 'f0864_41', 'f0958_28', 'f0933_38']\n"
     ]
    }
   ],
   "source": [
    "print(\"To Be Used: [{}]: {}\".format(len(workers_idx_to_be_used), workers_idx_to_be_used))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
